{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar-10-mlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aerjayc/coe197z-hw1/blob/master/cifar_10_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VVHwkt0uORdR"
      },
      "source": [
        "First, we download the `CIFAR-10` dataset and extract its contents to the current directory. The working directory is now:\n",
        "\n",
        "\n",
        "```\n",
        ">ls\n",
        "  cifar-10-batches-py  cifar-10-python.tar.gz  sample_data\n",
        ">ls cifar-10-batches-py\n",
        "  batches.meta  data_batch_2  data_batch_4  readme.html\n",
        "  data_batch_1  data_batch_3  data_batch_5  test_batch\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBHS-YoKCpV8",
        "colab_type": "code",
        "outputId": "91efde2b-d11c-440c-e2a3-770bb7eb92f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xzvf cifar-10-python.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-14 16:49:48--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  13.9MB/s    in 13s     \n",
            "\n",
            "2019-09-14 16:50:03 (12.1 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR8sSc0NZL3e",
        "colab_type": "text"
      },
      "source": [
        "Then, we \"unpickle\" the downloaded dataset. The `unpickle()` function was taken from the [CIFAR website](http://www.cs.toronto.edu/~kriz/cifar.html). It takes the filename of any of the batch files, and outputs the `dict` contained in that file.\n",
        "The structure of these `dict`'s is the following:\n",
        "\n",
        "\n",
        "```\n",
        "batch = {\n",
        "  b'batch_label': b'training batch 1 of 5',\n",
        "  b'data': array([ [<image_0>],\n",
        "                   ...,\n",
        "                   [<image 9999>]\n",
        "                 ], dtype=uint8),\n",
        "  b'filenames': [b'<filename_0.png',\n",
        "                 ...,\n",
        "                 b'<filename_9999.png',\n",
        "                ],\n",
        "  b'labels': [<label_0>, ..., <label_9999>],\n",
        "}\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "*   `[<image i>]` is a numpy vector of shape `(1024*1024*3,)`. The first `1024` entries in the vector correspond to the pixel intensities of the red channel, the second `1024` corresponds to the blue channel, and so on. Note that the pixel intensity spans the entire uint8 values.\n",
        "*   `b'<filename_i>'` is a binary string corresponding to the filename of the `i`-th image\n",
        "*   `<label_i>` $\\in \\{0,1,...,9\\}$, where each integer corresponds to some classification of the `i`-th image\n",
        "\n",
        "Note that the dictionary keys are binary strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOsCAN0uT7w-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpickle(file):\n",
        "  import pickle\n",
        "  with open(file, 'rb') as fo:\n",
        "    dict = pickle.load(fo, encoding='bytes')\n",
        "  return dict\n",
        "\n",
        "data_batch_1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
        "data_batch_2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
        "data_batch_3 = unpickle('cifar-10-batches-py/data_batch_3')\n",
        "data_batch_4 = unpickle('cifar-10-batches-py/data_batch_4')\n",
        "test_batch   = unpickle('cifar-10-batches-py/test_batch')\n",
        "# x1 = data_batch_1[b'data']\n",
        "# y1 = data_batch_1[b'labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYPDqOynsA16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.concatenate((data_batch_1[b'data'],\n",
        "              data_batch_2[b'data'],\n",
        "              data_batch_3[b'data'],\n",
        "              data_batch_4[b'data']))\n",
        "y = np.concatenate((data_batch_1[b'labels'],\n",
        "              data_batch_2[b'labels'],\n",
        "              data_batch_3[b'labels'],\n",
        "              data_batch_4[b'labels']))\n",
        "\n",
        "x_test = test_batch[b'data']\n",
        "y_test = test_batch[b'labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AiIG_tnM_6n",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9pOirjuBGHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_set(data, labels, data_fraction=0.8):\n",
        "  assert 0 < data_fraction <= 1\n",
        "\n",
        "  # jointly shuffle the data and labels first\n",
        "  # note: data and labels get shuffled (not copied)\n",
        "  rng_state = np.random.get_state()\n",
        "  np.random.shuffle(data)\n",
        "  np.random.set_state(rng_state)\n",
        "  np.random.shuffle(labels)\n",
        "\n",
        "  # get the first data_fraction of the train set\n",
        "  boundary = int(len(data)*data_fraction)\n",
        "  train_data   = data[:boundary]\n",
        "  train_labels = labels[:boundary]\n",
        "  validation_data = data[boundary:]\n",
        "  validation_labels = labels[boundary:]\n",
        "\n",
        "  return (train_data, train_labels), (validation_data, validation_labels)\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = split_train_set(x, y)\n",
        "\n",
        "# normalization, vectorization\n",
        "from keras.utils import to_categorical\n",
        "x_train = x_train.astype('float32') / 255\n",
        "y_train = to_categorical(y_train)\n",
        "x_val = x_val.astype('float32') / 255\n",
        "y_val = to_categorical(y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETRxZJsWOYJa",
        "colab_type": "code",
        "outputId": "16e2ac87-8cac-4f12-c41c-79632af40f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "# input parameters\n",
        "input_shape = x_train[0].shape\n",
        "num_labels = 10\n",
        "\n",
        "# network parameters\n",
        "batch_size = 64\n",
        "hidden_units = 512\n",
        "data_augmentation = False\n",
        "epochs = 30\n",
        "max_batches = len(x_train) // batch_size\n",
        "\n",
        "dropout_rate = 0.05\n",
        "l2_weight = 0.001\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(hidden_units,\n",
        "                       kernel_regularizer=regularizers.l2(l2_weight),\n",
        "                       activation='relu', \n",
        "                       input_shape=input_shape))\n",
        "model.add(layers.Dropout(dropout_rate))\n",
        "model.add(layers.Dense(hidden_units,\n",
        "                       kernel_regularizer=regularizers.l2(l2_weight),\n",
        "                       activation='relu'))\n",
        "model.add(layers.Dropout(dropout_rate))\n",
        "model.add(layers.Dense(num_labels, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_52 (Dense)             (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,841,162\n",
            "Trainable params: 1,841,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8kH2HKPjLRB",
        "colab_type": "code",
        "outputId": "a0bc0ab0-45bc-4dd1-b961-a582214e89d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# training the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs, batch_size=batch_size,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/50\n",
            "32000/32000 [==============================] - 4s 117us/step - loss: 2.3043 - acc: 0.2584 - val_loss: 2.1142 - val_acc: 0.2766\n",
            "Epoch 2/50\n",
            "32000/32000 [==============================] - 3s 84us/step - loss: 1.9041 - acc: 0.3311 - val_loss: 1.8311 - val_acc: 0.3596\n",
            "Epoch 3/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.8518 - acc: 0.3557 - val_loss: 1.8546 - val_acc: 0.3563\n",
            "Epoch 4/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.8153 - acc: 0.3725 - val_loss: 1.7500 - val_acc: 0.4078\n",
            "Epoch 5/50\n",
            "32000/32000 [==============================] - 3s 84us/step - loss: 1.7808 - acc: 0.3849 - val_loss: 1.7355 - val_acc: 0.4051\n",
            "Epoch 6/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.7619 - acc: 0.3936 - val_loss: 1.7435 - val_acc: 0.4076\n",
            "Epoch 7/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.7456 - acc: 0.3976 - val_loss: 1.6959 - val_acc: 0.4150\n",
            "Epoch 8/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.7354 - acc: 0.4019 - val_loss: 1.6919 - val_acc: 0.4274\n",
            "Epoch 9/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.7232 - acc: 0.4140 - val_loss: 1.6753 - val_acc: 0.4345\n",
            "Epoch 10/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.7205 - acc: 0.4146 - val_loss: 1.7202 - val_acc: 0.4079\n",
            "Epoch 11/50\n",
            "32000/32000 [==============================] - 3s 84us/step - loss: 1.7131 - acc: 0.4152 - val_loss: 1.7223 - val_acc: 0.4071\n",
            "Epoch 12/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.7121 - acc: 0.4182 - val_loss: 1.6726 - val_acc: 0.4375\n",
            "Epoch 13/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.7065 - acc: 0.4229 - val_loss: 1.6689 - val_acc: 0.4361\n",
            "Epoch 14/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.7017 - acc: 0.4233 - val_loss: 1.6619 - val_acc: 0.4490\n",
            "Epoch 15/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.6991 - acc: 0.4241 - val_loss: 1.7291 - val_acc: 0.4066\n",
            "Epoch 16/50\n",
            "32000/32000 [==============================] - 3s 87us/step - loss: 1.6962 - acc: 0.4251 - val_loss: 1.7188 - val_acc: 0.4169\n",
            "Epoch 17/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.6949 - acc: 0.4264 - val_loss: 1.7729 - val_acc: 0.4056\n",
            "Epoch 18/50\n",
            "32000/32000 [==============================] - 3s 84us/step - loss: 1.6872 - acc: 0.4263 - val_loss: 1.6943 - val_acc: 0.4323\n",
            "Epoch 19/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.6916 - acc: 0.4304 - val_loss: 1.6387 - val_acc: 0.4537\n",
            "Epoch 20/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.6874 - acc: 0.4292 - val_loss: 1.6764 - val_acc: 0.4334\n",
            "Epoch 21/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.6871 - acc: 0.4295 - val_loss: 1.7189 - val_acc: 0.4329\n",
            "Epoch 22/50\n",
            "32000/32000 [==============================] - 3s 87us/step - loss: 1.6886 - acc: 0.4278 - val_loss: 1.7181 - val_acc: 0.4219\n",
            "Epoch 23/50\n",
            "32000/32000 [==============================] - 3s 89us/step - loss: 1.6861 - acc: 0.4291 - val_loss: 1.7312 - val_acc: 0.4236\n",
            "Epoch 24/50\n",
            "32000/32000 [==============================] - 3s 88us/step - loss: 1.6896 - acc: 0.4319 - val_loss: 1.7073 - val_acc: 0.4400\n",
            "Epoch 25/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.6834 - acc: 0.4317 - val_loss: 1.6794 - val_acc: 0.4390\n",
            "Epoch 26/50\n",
            "32000/32000 [==============================] - 3s 87us/step - loss: 1.6750 - acc: 0.4382 - val_loss: 1.7230 - val_acc: 0.4231\n",
            "Epoch 27/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.6793 - acc: 0.4351 - val_loss: 1.6536 - val_acc: 0.4402\n",
            "Epoch 28/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.6792 - acc: 0.4351 - val_loss: 1.7058 - val_acc: 0.4210\n",
            "Epoch 29/50\n",
            "32000/32000 [==============================] - 3s 87us/step - loss: 1.6784 - acc: 0.4352 - val_loss: 1.8478 - val_acc: 0.3929\n",
            "Epoch 30/50\n",
            "32000/32000 [==============================] - 3s 87us/step - loss: 1.6794 - acc: 0.4336 - val_loss: 1.7357 - val_acc: 0.4255\n",
            "Epoch 31/50\n",
            "32000/32000 [==============================] - 3s 84us/step - loss: 1.6750 - acc: 0.4369 - val_loss: 1.7132 - val_acc: 0.4163\n",
            "Epoch 32/50\n",
            "32000/32000 [==============================] - 3s 88us/step - loss: 1.6766 - acc: 0.4365 - val_loss: 1.6862 - val_acc: 0.4306\n",
            "Epoch 33/50\n",
            "32000/32000 [==============================] - 3s 87us/step - loss: 1.6835 - acc: 0.4324 - val_loss: 1.7297 - val_acc: 0.4245\n",
            "Epoch 34/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.6789 - acc: 0.4404 - val_loss: 1.7398 - val_acc: 0.4099\n",
            "Epoch 35/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.6747 - acc: 0.4368 - val_loss: 1.7098 - val_acc: 0.4276\n",
            "Epoch 36/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.6760 - acc: 0.4335 - val_loss: 1.7720 - val_acc: 0.4166\n",
            "Epoch 37/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.6688 - acc: 0.4376 - val_loss: 1.6832 - val_acc: 0.4303\n",
            "Epoch 38/50\n",
            "32000/32000 [==============================] - 3s 82us/step - loss: 1.6757 - acc: 0.4384 - val_loss: 1.7726 - val_acc: 0.4012\n",
            "Epoch 39/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.6724 - acc: 0.4398 - val_loss: 1.7022 - val_acc: 0.4299\n",
            "Epoch 40/50\n",
            "32000/32000 [==============================] - 3s 81us/step - loss: 1.6730 - acc: 0.4413 - val_loss: 1.7079 - val_acc: 0.4213\n",
            "Epoch 41/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.6702 - acc: 0.4399 - val_loss: 1.6421 - val_acc: 0.4476\n",
            "Epoch 42/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.6733 - acc: 0.4436 - val_loss: 1.6670 - val_acc: 0.4400\n",
            "Epoch 43/50\n",
            "32000/32000 [==============================] - 3s 87us/step - loss: 1.6746 - acc: 0.4358 - val_loss: 1.7343 - val_acc: 0.4163\n",
            "Epoch 44/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.6742 - acc: 0.4411 - val_loss: 1.7450 - val_acc: 0.4191\n",
            "Epoch 45/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.6811 - acc: 0.4376 - val_loss: 1.7425 - val_acc: 0.4091\n",
            "Epoch 46/50\n",
            "32000/32000 [==============================] - 3s 83us/step - loss: 1.6764 - acc: 0.4382 - val_loss: 1.7204 - val_acc: 0.4293\n",
            "Epoch 47/50\n",
            "32000/32000 [==============================] - 3s 84us/step - loss: 1.6742 - acc: 0.4383 - val_loss: 1.7268 - val_acc: 0.4226\n",
            "Epoch 48/50\n",
            "32000/32000 [==============================] - 3s 86us/step - loss: 1.6720 - acc: 0.4403 - val_loss: 1.7794 - val_acc: 0.4402\n",
            "Epoch 49/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.6713 - acc: 0.4402 - val_loss: 1.6886 - val_acc: 0.4363\n",
            "Epoch 50/50\n",
            "32000/32000 [==============================] - 3s 85us/step - loss: 1.6709 - acc: 0.4412 - val_loss: 1.7080 - val_acc: 0.4325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZnwNbcDiw8d",
        "colab_type": "code",
        "outputId": "be82b92e-ff74-43ff-8434-430c0d858a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZKkWWm2hVBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting the training and validation losses\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(history):\n",
        "  # assert 'val_loss' in history.history\n",
        "\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epoch_axis = range(1, epochs + 1)\n",
        "\n",
        "  plt.plot(epoch_axis, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epoch_axis, val_loss, 'b', label='Validation loss')\n",
        "\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4gCJvq5hdri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting the training and validation accuracy\n",
        "def plot_acc(history):\n",
        "  # assert 'val_acc' in history.history\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "\n",
        "  plt.plot(epochs, acc, 'ro', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "\n",
        "  plt.title('Training and validation loss & acc')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Acc')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}